{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdY9AyAVWX7XOltSFZxt8Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheul0518/DeepLearning/blob/main/BME595/Homework01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0HGpaYUHpG6",
        "outputId": "3f8f3730-22d3-4c14-ce21-7e144d4a5b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part A"
      ],
      "metadata": {
        "id": "THX0JzbbHO3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class Conv2D(nn.Module):\n",
        "      def __init__(self, in_channel: int, o_channel: int, kernel_size: int, stride: int, mode: str):\n",
        "          super().__init__()\n",
        "          self.in_channel = in_channel\n",
        "          self.o_channel = o_channel\n",
        "          self.kernel_size = kernel_size\n",
        "          self.stride = stride\n",
        "          self.mode = mode\n",
        "          self.repos = []\n",
        "\n",
        "      def forward(self, input_image: PIL):\n",
        "          # Do convolution computation: size of height/width: (H – F + 2P) / S + 1\n",
        "          # Transform PIL (image) to Tensor (image)\n",
        "          transform = transforms.Compose([transforms.ToTensor()])\n",
        "          img = transform(input_image)\n",
        "          # Height, Width, after convolution\n",
        "          s = self.stride\n",
        "          c = self.o_channel # channels\n",
        "          h = int((img.shape[1] - self.kernel_size + 2*0) / self.stride + 1) # height\n",
        "          w = int((img.shape[2] - self.kernel_size + 2*0) / self.stride + 1) # width\n",
        "          conv = torch.zeros(c, h, w)\n",
        "          for o in range(self.o_channel):\n",
        "              for i in range(conv.shape[0]):\n",
        "                  for j in range(conv.shape[1]):\n",
        "                      for k in range(conv.shape[2]):\n",
        "                          conv[o,j,k] = conv[o,j,k] + (img[i, j*s:j*s+self.kernel()[o].shape[0], k*s:k*s+self.kernel()[o].shape[1]] * self.kernel()[o]).sum()\n",
        "          \n",
        "          # return conv\n",
        "          #print(F.conv2d(input=img, weight=self.kernel(), stride=1))\n",
        "          #print(img.shape)\n",
        "          a = torch.reshape(self.kernel(),(3,3))\n",
        "          b = torch.stack((a,a,a))\n",
        "          b = b.type(torch.float)\n",
        "          print(conv == (F.conv2d(input=img.reshape(1,img.shape[0],img.shape[1],img.shape[2]), weight=b.reshape(1, 3, 3, 3), bias=None, stride=1, padding=0, dilation=1, groups=1)))\n",
        "          \n",
        "\n",
        "\n",
        "      def kernel(self):\n",
        "          k1 = torch.tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
        "          k2 = torch.tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "          k3 = torch.tensor([[ 1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
        "          k4 = torch.tensor([[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]])\n",
        "          k5 = torch.tensor([[-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1]])\n",
        "          if self.mode == 'known':\n",
        "            if self.o_channel == 1:\n",
        "                return torch.reshape(k1, (1, k1.shape[0], k1.shape[1]))\n",
        "            elif self.o_channel == 2:\n",
        "                return torch.stack((k4,k5))\n",
        "            else:\n",
        "                return torch.stack((k1,k2,k3))\n",
        "          elif self.mode == 'rand':\n",
        "                return torch.rand((self.o_channel, self.kernel_size, self.kernel_size))\n",
        "          else:\n",
        "                return None"
      ],
      "metadata": {
        "id": "BXNHWrbIrSi-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  # Part A\n",
        "  conv2d = Conv2D(in_channel=3, o_channel=1, kernel_size=3, stride=1, mode='known')\n",
        "  img = Image.open('/content/drive/MyDrive/DeepLearning/BME595/ironman1280720.jpg')\n",
        "  conv = conv2d.forward(input_image = img)\n",
        "  #print(conv2d.kernel().shape,'\\n\\n', conv2d.kernel())\n",
        "  #a = torch.reshape(conv2d.kernel(),(1,conv2d.kernel().shape[0],conv2d.kernel().shape[1]))"
      ],
      "metadata": {
        "id": "qI2P47TSPbiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k1 = torch.tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
        "k2 = torch.tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "k3 = torch.tensor([[ 1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
        "k4 = torch.tensor([[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]])\n",
        "k5 = torch.tensor([[-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1], [-1, -1, 0, 1, 1]])\n",
        "\n",
        "k1 = torch.reshape(k1, (1, k1.shape[0], k1.shape[1]))\n",
        "k1 = k1.view(1,k1.shape[0], k1.shape[1], k1.shape[2])\n",
        "print(k1.shape)\n",
        "print(conv.shape)\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_trans = transform(img)\n",
        "#img_trans = img_trans.view(1, img_trans.shape[0],img_trans.shape[1],img_trans.shape[2])\n",
        "#print(F.conv2d(input=img_trans, weight=k1))\n",
        "#print((conv == F.conv2d(input=img_trans, weight=k1)).sum() == conv.shape[0]*conv.shape[1]*conv.shape[2])\n",
        "total = torch.zeros((1,718,1278))\n",
        "k1 = k1.type(torch.float)\n",
        "for i in range(img_trans.shape[0]):\n",
        "    z = img_trans[i].view(1, 1, img_trans[i].shape[0], img_trans[i].shape[1])\n",
        "    total = total + F.conv2d(input = z, weight = k1)\n",
        "total = total.reshape((1,718,1278))\n",
        "print(conv == total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h18KxhMknSX",
        "outputId": "50df95ba-f986-4676-862b-115082c40eba"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 3, 3])\n",
            "torch.Size([1, 718, 1278])\n",
            "tensor([[[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "#plt.imshow(img)\n",
        "print(type(img))\n",
        "trans = transforms.Compose([transforms.ToTensor()])\n",
        "img_trans = trans(img)\n",
        "print(img_trans.shape)\n",
        "print(img_trans[0].shape)\n",
        "#plt.imshow(np.transpose(img_trans, (1,2,0)))\n",
        "plt.imshow(img_trans[2])\n",
        "\n",
        "# width, height = img.size\n",
        "# print(width, height)\n",
        "#img_tensor = torch.Tensor(img).to_tensor()"
      ],
      "metadata": {
        "id": "Oeet2BWNQScb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
        "b = torch.tensor([[1,2],[4,5]])\n",
        "\n",
        "# (W – F + 2P) / S + 1\n",
        "h = a.shape[0]\n",
        "w = a.shape[1]\n",
        "print(h,w)\n",
        "\n",
        "\n",
        "print(a[:2,:2], '\\n\\n', b, '\\n\\n', a[:2,:2] * b, '\\n\\n', torch.matmul(a[:2,:2],b),'\\n\\n', a[:2,:2] @ b,'\\n\\n', torch.mm(a[:2,:2], b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szs1eo5pZL2P",
        "outputId": "ee3dd1dd-5707-4988-d7bd-213245bbf73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 3\n",
            "tensor([[1, 2],\n",
            "        [4, 5]]) \n",
            "\n",
            " tensor([[1, 2],\n",
            "        [4, 5]]) \n",
            "\n",
            " tensor([[ 1,  4],\n",
            "        [16, 25]]) \n",
            "\n",
            " tensor([[ 9, 12],\n",
            "        [24, 33]]) \n",
            "\n",
            " tensor([[ 9, 12],\n",
            "        [24, 33]]) \n",
            "\n",
            " tensor([[ 9, 12],\n",
            "        [24, 33]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand((8,7))\n",
        "b = -torch.ones((2,2))\n",
        "s = 2\n",
        "h = int((a.shape[0] - b.shape[0] +2*0)/s + 1)\n",
        "w = int((a.shape[1] - b.shape[1] +2*0)/s + 1)\n",
        "c = torch.zeros((h,w))\n",
        "\n",
        "print(a, '\\n\\n', b)\n",
        "print('\\n\\n', c)\n",
        "for i in range(c.shape[0]):\n",
        "    for j in range(c.shape[1]):\n",
        "        c[i,j] = (a[i*s:i*s+b.shape[0],j*s:j*s+b.shape[1]]*b).sum()\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLQLX4Dsjiy0",
        "outputId": "0dafb01f-99d7-403a-f43d-bf81a60dcd54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7395, 0.7864, 0.6659, 0.6644, 0.6232, 0.4127, 0.8050],\n",
            "        [0.0448, 0.6490, 0.0493, 0.8421, 0.9548, 0.6959, 0.6195],\n",
            "        [0.1866, 0.0436, 0.5386, 0.1172, 0.3958, 0.8312, 0.8699],\n",
            "        [0.2881, 0.5981, 0.8526, 0.6223, 0.1749, 0.6002, 0.3097],\n",
            "        [0.0368, 0.1093, 0.5377, 0.7068, 0.7558, 0.8756, 0.1982],\n",
            "        [0.1433, 0.3499, 0.1206, 0.4696, 0.6780, 0.3501, 0.8097],\n",
            "        [0.7789, 0.5764, 0.2899, 0.2655, 0.9193, 0.3716, 0.2269],\n",
            "        [0.7860, 0.1281, 0.5304, 0.9580, 0.3930, 0.6692, 0.9114]]) \n",
            "\n",
            " tensor([[-1., -1.],\n",
            "        [-1., -1.]])\n",
            "\n",
            "\n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[-2.2197, -2.2218, -2.6865],\n",
            "        [-1.1164, -2.1307, -2.0021],\n",
            "        [-0.6393, -1.8347, -2.6595],\n",
            "        [-2.2694, -2.0438, -2.3531]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "a = a.view(1,1,8,7)\n",
        "b = b.view(1,1,2,2)\n",
        "test = F.conv2d(input=a, weight=b, stride=2)\n",
        "print(test)\n",
        "\n",
        "print((c == test).sum() == c.shape[0] * c.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6olpZbt4g0b",
        "outputId": "edc98ecf-2fef-451b-cf5f-dbf41abd2d53"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-2.2197, -2.2218, -2.6865],\n",
            "          [-1.1164, -2.1307, -2.0021],\n",
            "          [-0.6393, -1.8347, -2.6595],\n",
            "          [-2.2694, -2.0438, -2.3531]]]])\n",
            "tensor(True)\n"
          ]
        }
      ]
    }
  ]
}
